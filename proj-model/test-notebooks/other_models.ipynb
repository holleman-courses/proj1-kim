{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, DepthwiseConv2D, BatchNormalization, Activation, Dropout\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "def improved_tinyml_mobilenet():\n",
    "    input_shape = (96, 96, 1)  # Grayscale Input\n",
    "    num_classes = 1  # Binary classification (Dog vs Not-Dog)\n",
    "    num_filters = 16  # Start with more filters\n",
    "\n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    # **1st Layer: Convolution**\n",
    "    x = Conv2D(num_filters, (3,3), strides=2, padding='same', \n",
    "               kernel_initializer='he_normal', kernel_regularizer=l2(1e-5), use_bias=False)(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('swish')(x)  # âœ… Swish works better than ReLU\n",
    "\n",
    "    # **2nd Layer: Depthwise Separable Convolution**\n",
    "    x = DepthwiseConv2D((3,3), strides=1, padding='same', \n",
    "                         depthwise_initializer='he_normal', depthwise_regularizer=l2(1e-5), use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('swish')(x)\n",
    "\n",
    "    num_filters *= 2  # Increase filters\n",
    "    x = Conv2D(num_filters, (1,1), strides=1, padding='same', \n",
    "               kernel_initializer='he_normal', kernel_regularizer=l2(1e-5), use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('swish')(x)\n",
    "\n",
    "    # **3rd Layer: Depthwise Separable Convolution**\n",
    "    x = DepthwiseConv2D((3,3), strides=2, padding='same', \n",
    "                         depthwise_initializer='he_normal', depthwise_regularizer=l2(1e-5), use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('swish')(x)\n",
    "\n",
    "    num_filters *= 2  # Increase filters\n",
    "    x = Conv2D(num_filters, (1,1), strides=1, padding='same', \n",
    "               kernel_initializer='he_normal', kernel_regularizer=l2(1e-5), use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('swish')(x)\n",
    "\n",
    "    # **4th Layer: Depthwise Separable Convolution**\n",
    "    x = DepthwiseConv2D((3,3), strides=1, padding='same', \n",
    "                         depthwise_initializer='he_normal', depthwise_regularizer=l2(1e-5), use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('swish')(x)\n",
    "\n",
    "    # **5th Layer: Depthwise Separable Convolution**\n",
    "    x = DepthwiseConv2D((3,3), strides=2, padding='same', \n",
    "                         depthwise_initializer='he_normal', depthwise_regularizer=l2(1e-5), use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('swish')(x)\n",
    "\n",
    "    num_filters *= 2  # Increase filters\n",
    "    x = Conv2D(num_filters, (1,1), strides=1, padding='same', \n",
    "               kernel_initializer='he_normal', kernel_regularizer=l2(1e-5), use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('swish')(x)\n",
    "\n",
    "    # **Global Average Pooling**\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "\n",
    "    # **Fully Connected Layers**\n",
    "    x = Dense(256, activation='swish', kernel_regularizer=l2(1e-5))(x)  # âœ… Increased neurons\n",
    "    x = Dropout(0.4)(x)  # âœ… Increased dropout for better regularization\n",
    "    x = Dense(128, activation='swish', kernel_regularizer=l2(1e-5))(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "\n",
    "    # **Output Layer**\n",
    "    outputs = Dense(1, activation='sigmoid')(x)  # âœ… Binary classification (Dog vs Not-Dog)\n",
    "\n",
    "    # **Define Model**\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    return model\n",
    "\n",
    "# **Compile the Model**\n",
    "model = improved_tinyml_mobilenet()\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005), \n",
    "              loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# **Display Model Summary**\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Image size and batch size\n",
    "IMG_SIZE = (96, 96)\n",
    "BATCH_SIZE = 16  # ðŸ”¥ Reduced for better generalization\n",
    "EPOCHS = 30  # Will stop early if needed\n",
    "\n",
    "# Define early stopping to prevent overfitting\n",
    "early_stopping = EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True)\n",
    "\n",
    "# Compute class weights based on dataset size\n",
    "class_counts = np.array([11438 + 2818, 23947 + 6029])  # Total count for each class\n",
    "classes = np.array([0, 1])  # Labels: Not-Dog (0), Dog (1)\n",
    "\n",
    "class_weights = compute_class_weight(\"balanced\", classes=classes, y=np.repeat(classes, class_counts))\n",
    "class_weights = {i: w for i, w in enumerate(class_weights)}\n",
    "\n",
    "# ðŸ”¥ Reduce imbalance effect slightly\n",
    "class_weights[0] *= 0.8  # Reduce Not-Dog weight\n",
    "class_weights[1] *= 1.2  # Increase Dog weight\n",
    "print(f\"Updated Class Weights: {class_weights}\")\n",
    "\n",
    "# ðŸ”¥ Learning Rate Scheduler (decays LR every 5 epochs)\n",
    "def lr_scheduler(epoch, lr):\n",
    "    if epoch > 5:\n",
    "        return lr * 0.8  # Reduce LR by 20% every 5 epochs\n",
    "    return lr\n",
    "\n",
    "lr_callback = LearningRateScheduler(lr_scheduler)\n",
    "\n",
    "# Define Model\n",
    "model = improved_tinyml_mobilenet()  # Call your updated model function\n",
    "\n",
    "# Compile the Model\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005),  # ðŸ”¥ Reduced LR for smoother learning\n",
    "    loss=\"binary_crossentropy\", \n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_images, train_labels,\n",
    "    validation_data=(val_images, val_labels),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    class_weight=class_weights,  # Apply class balancing\n",
    "    callbacks=[early_stopping, lr_callback]  # ðŸ”¥ Include LR scheduler\n",
    ")\n",
    "\n",
    "# Save model\n",
    "MODEL_SAVE_PATH = \"New_Model_Update.h5\"\n",
    "model.save(MODEL_SAVE_PATH)\n",
    "print(f\"âœ… Model training complete! Saved as {MODEL_SAVE_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.utils.class_weight import compute_class_weight  # Correct import\n",
    "\n",
    "\n",
    "# Image size and batch size\n",
    "IMG_SIZE = (96, 96)\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 30  # Will stop early if needed\n",
    "\n",
    "# Define early stopping to prevent overfitting\n",
    "early_stopping = EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True)\n",
    "\n",
    "# Compute class weights based on dataset size\n",
    "class_counts = np.array([11438 + 2818, 23947 + 6029])  # Total count for each class\n",
    "classes = np.array([0, 1])  # Labels: Not-Dog (0), Dog (1)\n",
    "\n",
    "class_weights = compute_class_weight(\"balanced\", classes=classes, y=np.repeat(classes, class_counts))\n",
    "class_weights = {i: w for i, w in enumerate(class_weights)}\n",
    "\n",
    "# Print computed class weights\n",
    "print(f\"Computed Class Weights: {class_weights}\")\n",
    "\n",
    "# Define Model\n",
    "model = improved_tinyml_mobilenet()  # Call your updated model function\n",
    "\n",
    "# Compile the Model\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), \n",
    "    loss=\"binary_crossentropy\", \n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_images, train_labels,\n",
    "    validation_data=(val_images, val_labels),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    class_weight=class_weights,  # Apply class balancing\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "# Save model\n",
    "MODEL_SAVE_PATH = \"New_Model_Update.h5\"\n",
    "model.save(MODEL_SAVE_PATH)\n",
    "print(f\"âœ… Model training complete! Saved as {MODEL_SAVE_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, DepthwiseConv2D, BatchNormalization, Activation, Dropout\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "def improved_tinyml_mobilenet():\n",
    "    input_shape = (96, 96, 1)  # Grayscale Input\n",
    "    num_classes = 1  # Binary classification (Dog vs Not-Dog)\n",
    "    num_filters = 16  # ðŸ”¥ Increased from 8 to 16\n",
    "\n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    # **1st Layer: Convolution**\n",
    "    x = Conv2D(num_filters, (3,3), strides=2, padding='same', \n",
    "               kernel_initializer='he_normal', kernel_regularizer=l2(5e-5), use_bias=False)(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)  # âœ… Using ReLU\n",
    "\n",
    "    # **2nd Layer: Depthwise Separable Convolution**\n",
    "    x = DepthwiseConv2D((3,3), strides=1, padding='same', \n",
    "                         depthwise_initializer='he_normal', depthwise_regularizer=l2(5e-5), use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)  # âœ… Using ReLU\n",
    "\n",
    "    num_filters *= 2  # ðŸ”¥ Increase filters\n",
    "    x = Conv2D(num_filters, (1,1), strides=1, padding='same', \n",
    "               kernel_initializer='he_normal', kernel_regularizer=l2(5e-5), use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)  # âœ… Using ReLU\n",
    "\n",
    "    # **3rd Layer: Depthwise Separable Convolution**\n",
    "    x = DepthwiseConv2D((3,3), strides=2, padding='same', \n",
    "                         depthwise_initializer='he_normal', depthwise_regularizer=l2(5e-5), use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)  # âœ… Using ReLU\n",
    "\n",
    "    num_filters *= 2  # ðŸ”¥ Increase filters\n",
    "    x = Conv2D(num_filters, (1,1), strides=1, padding='same', \n",
    "               kernel_initializer='he_normal', kernel_regularizer=l2(5e-5), use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)  # âœ… Using ReLU\n",
    "\n",
    "    # **4th Layer: Depthwise Separable Convolution**\n",
    "    x = DepthwiseConv2D((3,3), strides=1, padding='same', \n",
    "                         depthwise_initializer='he_normal', depthwise_regularizer=l2(5e-5), use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)  # âœ… Using ReLU\n",
    "\n",
    "    # **5th Layer: Depthwise Separable Convolution**\n",
    "    x = DepthwiseConv2D((3,3), strides=2, padding='same', \n",
    "                         depthwise_initializer='he_normal', depthwise_regularizer=l2(5e-5), use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)  # âœ… Using ReLU\n",
    "\n",
    "    num_filters *= 2  # ðŸ”¥ Increase filters\n",
    "    x = Conv2D(num_filters, (1,1), strides=1, padding='same', \n",
    "               kernel_initializer='he_normal', kernel_regularizer=l2(5e-5), use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)  # âœ… Using ReLU\n",
    "\n",
    "    # **ðŸ”¥ Extra Conv Layer Before GAP to Improve Feature Learning**\n",
    "    x = Conv2D(num_filters, (3,3), padding='same', \n",
    "               kernel_initializer='he_normal', kernel_regularizer=l2(5e-5), use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)  # âœ… Using ReLU\n",
    "\n",
    "    # **Global Average Pooling**\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "\n",
    "    # **Fully Connected Layers**\n",
    "    x = Dense(256, activation='relu', kernel_regularizer=l2(5e-5))(x)  # ðŸ”¥ Increased from 128 â†’ 256\n",
    "    x = Dropout(0.4)(x)  # ðŸ”¥ Increased dropout to improve generalization\n",
    "\n",
    "    # **Output Layer**\n",
    "    outputs = Dense(1, activation='sigmoid')(x)  # âœ… Binary classification (Dog vs Not-Dog)\n",
    "\n",
    "    # **Define Model**\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    return model\n",
    "\n",
    "model = improved_tinyml_mobilenet()\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005), \n",
    "              loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# **Display Model Summary**\n",
    "model.summary()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
